# Conversational AI over SQL Data: Enterprise CPG Solution
**Production-Ready Implementation with Semantic Layer Architecture**

---

## ğŸ“‹ Document Information

**Author:** Varun - Senior Data Engineer   
**Date:** February 2026  
**Version:** 1.0 - Production Ready  
**Approach:** Semantic Layer + AST (Zero LLM-Generated SQL)

---

## Table of Contents

1. [Summary](#executive-summary)
2. [Business Problem & Solution](#business-problem--solution)
3. [Why NOT Use LLMs for SQL Generation](#why-not-use-llms-for-sql-generation)
4. [Complete Solution Architecture](#complete-solution-architecture)
5. [Semantic Layer Foundation](#semantic-layer-foundation)
6. [CPG Industry Implementation](#cpg-industry-implementation)
7. [Query Intelligence System](#query-intelligence-system)
8. [Multi-Query Diagnostic Engine](#multi-query-diagnostic-engine)
9. [Security & Governance Framework](#security--governance-framework)
10. [Complete Technical Implementation](#complete-technical-implementation)
11. [Production Deployment Guide](#production-deployment-guide)
12. [Business Case & ROI](#business-case--roi)

---

## 1. Summary

### The Challenge

Sales organizations face a critical gap between data availability and data accessibility:
- Sales teams spend **8-10 hours per week** creating manual reports
- Traditional BI tools require extensive training and aren't field-friendly
- SQL is too technical for business users
- LLM-generated SQL is unsafe, inconsistent, and ungovernable

### Our Solution

A **Conversational Analytics Platform** specifically designed for CPG enterprises that:

âœ… **Speaks Business Language** - Natural questions like "Why did beverage sales drop in the Northeast?"  
âœ… **100% Safe** - No SQL injection, no hallucinations, deterministic results  
âœ… **Governed by Design** - Single source of truth, audit trails, role-based access  
âœ… **Strategic Insights** - Multi-query diagnostic workflows for root cause analysis  
âœ… **CPG-Native** - Secondary sales, velocity, distribution, fiscal calendars built-in  

### Architecture Philosophy

```
Natural Language Question
        â†“
[LLM] Intent Recognition â†’ Structured JSON
        â†“
[Semantic Layer] Business Logic & Validation
        â†“
[AST Builder] Deterministic SQL Generation
        â†“
[Data Warehouse] Optimized Execution
        â†“
[Response Generator] Natural Language + Visualization
```

**Key Principle:** LLMs understand questions, Semantic Layer generates queries.

### Expected Outcomes

- **Time Savings:** 8 hours/week per sales rep
- **Cost Avoidance:** $600K/week across 500 reps
- **Annual Value:** ~$31 Million
- **ROI:** 50x+ in Year 1
- **Adoption Rate:** >60% of sales team using weekly

---

## 2. Business Problem & Solution

### 2.1 Current State Pain Points

**For Sales Representatives:**
- Manual report creation takes hours
- Can't answer customer questions on the spot
- Wait for analyst support for custom queries
- Inconsistent metrics across reports

**For Sales Managers:**
- No real-time visibility into team performance
- Difficult to diagnose problems quickly
- Can't drill down into issues during calls
- Spend time validating reports instead of coaching

**For Analytics Teams:**
- Overwhelmed with ad-hoc report requests
- Same questions asked repeatedly
- No self-service capability
- Can't scale support

**For the Organization:**
- Inconsistent metric definitions
- Slow decision-making
- Competitive disadvantage
- High cost of data access

### 2.2 Why Traditional Solutions Fail

| Approach | Problem |
|----------|---------|
| **BI Tools (Tableau, Power BI)** | Require training, not mobile-friendly, not conversational |
| **Direct SQL Access** | Too technical, dangerous, no governance |
| **Pre-built Reports** | Inflexible, don't answer follow-up questions |
| **LLM â†’ SQL Tools** | Unsafe, inconsistent, hallucinate, can't audit |

### 2.3 Our Approach: Conversational Analytics Platform

This is NOT a chatbot. This is NOT "AI writes SQL."

This is a **governed, conversational interface** to your enterprise data with:

1. **Business Logic Firewall (Semantic Layer)**
   - Single source of truth for all metrics
   - Enforces joins, aggregations, security
   - Validates every query before execution

2. **Natural Language Understanding (LLM)**
   - Extracts intent from questions
   - Outputs structured JSON (not SQL)
   - Replaceable component

3. **Deterministic Query Generation (AST)**
   - No hallucination possible
   - Every query is testable
   - Full audit trail

4. **Strategic Intelligence (Diagnostic Workflows)**
   - "Why" questions trigger multi-query analysis
   - Root cause identification
   - Proactive insights

---

## 3. Why NOT Use LLMs for SQL Generation

### 3.1 The Seven Deadly Sins

| Risk | Impact | Example |
|------|--------|---------|
| **SQL Injection** | Security breach | `'; DROP TABLE users; --` |
| **Schema Hallucination** | Wrong results | `SELECT fake_column FROM non_existent_table` |
| **Join Logic Errors** | Data corruption | Missing foreign keys = Cartesian explosion |
| **Business Logic Bypass** | Compliance violation | Ignoring "exclude returns" rule |
| **Performance Disasters** | System crashes | Full table scan on 10 billion rows |
| **Non-Deterministic** | Trust erosion | Same question â†’ different SQL daily |
| **Ungovernable** | Audit failure | Can't explain how metric was calculated |

### 3.2 Real-World Example

**User Question:** "What were beverage sales last month?"

**âŒ LLM-Generated SQL (Dangerous):**
```sql
-- LLM might generate:
SELECT SUM(sales_amount) 
FROM sales 
WHERE category = 'Beverages'
  AND month = 'January'
```

**Problems:**
- Which sales? (Primary vs Secondary?)
- Includes returns
- String date comparison
- No territory filter
- Wrong table
- Inconsistent results

**âœ… Our Approach (Safe & Governed):**

```
Step 1: LLM extracts intent
{
  "metric": "secondary_sales_value",
  "filters": {"category": "Beverages"},
  "time_range": "last_month"
}

Step 2: Semantic Layer applies business rules
- Resolves "sales" â†’ secondary_sales_value
- Excludes returns automatically
- Uses fiscal calendar
- Applies user's territory filter
- Selects optimal table

Step 3: AST generates SQL
SELECT SUM(invoice_value) AS secondary_sales_value
FROM fact_secondary_sales f
JOIN dim_product p ON f.product_id = p.product_id
JOIN dim_time t ON f.date_id = t.date_id
JOIN dim_geo g ON f.geo_id = g.geo_id
WHERE p.category_name = 'Beverages'
  AND t.fiscal_month = 202401
  AND f.return_flag = 'N'
  AND f.transaction_type = 'Secondary'
  AND g.territory_id = 'USER_TERRITORY'  -- Auto-injected
```

**Result:**
- âœ… Correct metric (secondary sales)
- âœ… Excludes returns
- âœ… Uses fiscal calendar
- âœ… Applies security
- âœ… Optimized performance
- âœ… Consistent every time

### 3.3 Our Architectural Guarantee

**We guarantee that:**
- No SQL is generated by LLM
- Every query is validated before execution
- Business rules are always applied
- Security is enforced automatically
- Results are deterministic and auditable

---

## 4. Complete Solution Architecture

### 4.1 System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   USER INTERFACE LAYER                           â”‚
â”‚  Web App â”‚ Mobile App â”‚ MS Teams â”‚ Slack â”‚ Voice Interface      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTPS/REST API
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     API GATEWAY                                  â”‚
â”‚  â€¢ Authentication (JWT/OAuth)                                    â”‚
â”‚  â€¢ Rate Limiting                                                 â”‚
â”‚  â€¢ Load Balancing                                                â”‚
â”‚  â€¢ Request Routing                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           CONVERSATIONAL ANALYTICS ENGINE                        â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  1. INTENT RECOGNITION (LLM)                              â”‚ â”‚
â”‚  â”‚     â€¢ Parse natural language                               â”‚ â”‚
â”‚  â”‚     â€¢ Extract metrics, dimensions, filters                 â”‚ â”‚
â”‚  â”‚     â€¢ Classify intent type (trend/comparison/diagnostic)   â”‚ â”‚
â”‚  â”‚     â€¢ Output: Structured JSON (not SQL)                    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚               â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  2. SEMANTIC VALIDATION                                    â”‚ â”‚
â”‚  â”‚     â€¢ Resolve synonyms â†’ canonical names                   â”‚ â”‚
â”‚  â”‚     â€¢ Validate metric + dimension compatibility            â”‚ â”‚
â”‚  â”‚     â€¢ Apply business rules (exclude returns, etc)          â”‚ â”‚
â”‚  â”‚     â€¢ Inject security filters (row-level security)         â”‚ â”‚
â”‚  â”‚     â€¢ Determine required tables and joins                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚               â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  3. QUERY ORCHESTRATOR                                     â”‚ â”‚
â”‚  â”‚     â€¢ Single query â†’ AST Builder                           â”‚ â”‚
â”‚  â”‚     â€¢ Diagnostic intent â†’ Multi-query workflow             â”‚ â”‚
â”‚  â”‚     â€¢ Estimate cost & apply guardrails                     â”‚ â”‚
â”‚  â”‚     â€¢ Select optimal table (aggregated vs fact)            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚               â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  4. AST QUERY BUILDER                                      â”‚ â”‚
â”‚  â”‚     â€¢ Build Abstract Syntax Tree (not string concat)       â”‚ â”‚
â”‚  â”‚     â€¢ Generate dialect-specific SQL (Snowflake/Redshift)   â”‚ â”‚
â”‚  â”‚     â€¢ Apply optimizations                                  â”‚ â”‚
â”‚  â”‚     â€¢ Guarantee: No SQL injection possible                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚               â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  5. EXECUTION & CACHING                                    â”‚ â”‚
â”‚  â”‚     â€¢ Execute queries against data warehouse               â”‚ â”‚
â”‚  â”‚     â€¢ Cache results (semantic-aware)                       â”‚ â”‚
â”‚  â”‚     â€¢ Log for audit                                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚               â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  6. RESPONSE GENERATION (LLM)                              â”‚ â”‚
â”‚  â”‚     â€¢ Format results in natural language                   â”‚ â”‚
â”‚  â”‚     â€¢ Recommend visualizations                             â”‚ â”‚
â”‚  â”‚     â€¢ Generate insights                                    â”‚ â”‚
â”‚  â”‚     â€¢ Suggest follow-up questions                          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA LAYER                                  â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Fact Tables â”‚  â”‚  Aggregated  â”‚  â”‚   Dimension    â”‚        â”‚
â”‚  â”‚  (10B rows) â”‚  â”‚    Tables    â”‚  â”‚    Tables      â”‚        â”‚
â”‚  â”‚             â”‚  â”‚  (100M rows) â”‚  â”‚   (1M rows)    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                  â”‚
â”‚  Snowflake / Redshift / BigQuery / Databricks                   â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  CACHE LAYER (Redis)                                    â”‚   â”‚
â”‚  â”‚  â€¢ Query result cache                                   â”‚   â”‚
â”‚  â”‚  â€¢ Semantic-aware cache keys                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Component Descriptions

#### **1. Intent Recognition (LLM Component)**

**Purpose:** Convert natural language to structured intent

**Input:** "Show me beverage sales velocity by week in Tamil Nadu"

**Output:**
```json
{
  "intent_type": "trend",
  "metrics": ["sales_velocity"],
  "dimensions": ["fiscal_week"],
  "filters": [
    {"dimension": "category", "operator": "=", "value": "Beverages"},
    {"dimension": "state", "operator": "=", "value": "Tamil Nadu"}
  ],
  "time_range": {"relative": "last_13_weeks"},
  "sort_by": [{"field": "fiscal_week", "direction": "ASC"}]
}
```

**Key Points:**
- Uses Claude/GPT for natural language understanding
- Temperature=0 for deterministic output
- Validates against JSON schema
- No SQL generation at this stage

#### **2. Semantic Validation**

**Purpose:** Apply business logic and validate intent

**Actions:**
1. Resolve synonyms: "sales" â†’ "secondary_sales_value"
2. Validate compatibility: Can sales_velocity be grouped by week? Yes âœ“
3. Check permissions: Can user access Tamil Nadu data? Yes âœ“
4. Inject security: Add territory_id filter automatically
5. Determine joins: Need fact_sales + dim_product + dim_geo + dim_time

**Output:** Validated, enriched intent ready for query building

#### **3. Query Orchestrator**

**Purpose:** Route to appropriate execution strategy

**Decision Tree:**
```
if intent_type == "diagnostic":
    â†’ Multi-Query Workflow
        1. Trend analysis
        2. Contribution breakdown
        3. Distribution check
        4. Exception detection
else:
    â†’ Single Query via AST Builder
```

**Also:**
- Estimates query cost
- Selects optimal table (agg vs fact)
- Applies cost guardrails

#### **4. AST Query Builder**

**Purpose:** Generate SQL from validated intent (zero hallucination)

**Process:**
```
Validated Intent
    â†“
Build Abstract Syntax Tree
    - SELECT clause (metrics + dimensions)
    - FROM clause (fact table)
    - JOIN clauses (required dimensions)
    - WHERE clause (filters + security + business rules)
    - GROUP BY clause (dimensions)
    - ORDER BY clause
    - LIMIT clause
    â†“
Compile to SQL (dialect-specific)
    â†“
Parameterized SQL Query
```

**Guarantee:** SQL is deterministic. Same intent always produces same SQL.

#### **5. Execution & Caching**

**Purpose:** Execute queries efficiently

**Features:**
- Connection pooling
- Query result caching (Redis)
- Semantic-aware cache keys
- Execution time tracking
- Row-level security enforcement
- Audit logging

#### **6. Response Generation**

**Purpose:** Convert data back to natural language

**Input:** Query results (rows of data)

**Output:** 
```
"Beverage sales velocity in Tamil Nadu has been trending upward 
over the last 13 weeks, averaging 12.5 units per store per week. 
The strongest week was Week 5 with 15.3 units per store."

[Line chart visualization recommended]

Follow-up questions:
- Which brands drove this growth?
- How does this compare to other states?
- What was the distribution coverage?
```

---

## 5. Semantic Layer Foundation

### 5.1 What IS the Semantic Layer?

The semantic layer is the **business logic firewall** between users and data.

**It IS:**
- Metadata catalog (metrics, dimensions, relationships)
- Query compiler (intent â†’ SQL)
- Governance engine (security, validation)
- Single source of truth

**It is NOT:**
- A visualization tool
- A data transformation engine
- An LLM prompt

### 5.2 Core Components

#### A. Metric Store

Every metric defined once with complete business context:

```yaml
metric: secondary_sales_value
display_name: "Secondary Sales Value"
description: "Invoice value from distributor to retailer (excludes returns)"
formula: SUM(invoice_value)
fact_table: fact_secondary_sales
filters:
  - return_flag != 'Y'
  - transaction_type = 'Secondary'
time_dimension: invoice_date
grain: daily
aggregation: SUM
data_type: decimal(18,2)
format: "$:,.2f"
synonyms: ["sales", "revenue", "billing", "turnover"]
allowed_dimensions:
  - product (category, brand, sku)
  - geography (region, state, territory)
  - customer (distributor, channel)
  - time (day, week, month, quarter)
security:
  row_level: "territory_id IN (user.territories)"
last_updated: "2026-02-01"
```

**Why This Matters:**
- Change definition once â†’ affects all queries
- Self-documenting
- Prevents metric proliferation
- Enables governance

#### B. Dimension Catalog with Hierarchies

```yaml
dimension: product_hierarchy
type: hierarchy
levels:
  - level: category
    column: category_name
    cardinality: ~50
  
  - level: brand
    column: brand_name
    cardinality: ~500
    parent: category
  
  - level: sku
    column: sku_name
    cardinality: ~10000
    parent: brand

attributes:
  - launch_date
  - lifecycle_stage (new/growth/mature/decline)
  - price_tier (premium/mid/value)
  - velocity_segment (A/B/C)

synonyms:
  category: ["dept", "department", "product category"]
  brand: ["manufacturer", "make"]
  sku: ["product", "item"]
```

#### C. Time Intelligence

**Critical for CPG:** Fiscal calendars, promotional periods, rolling windows

```yaml
time_definitions:
  fiscal_year:
    start_month: 4  # April
    pattern: "4-4-5"  # weeks per period
    
  rolling_windows:
    - last_4_weeks
    - last_13_weeks
    - ytd (fiscal year to date)
    - mat (moving annual total - 12 months)
  
  comparison_periods:
    - vs_prior_year: offset -52 weeks
    - vs_prior_period: offset -1 period
    - vs_target: from planning_system
```

**Example:** "Last quarter" means Fiscal Q (13 weeks exactly), not Calendar Q (92-93 days)

#### D. Relationship Graph

```yaml
relationships:
  - from: fact_secondary_sales.product_id
    to: dim_product.product_id
    type: many_to_one
    
  - from: fact_secondary_sales.distributor_id
    to: dim_distributor.distributor_id
    type: many_to_one
    
  - from: dim_distributor.territory_id
    to: dim_territory.territory_id
    type: many_to_one

join_policies:
  forbidden_joins:
    - [fact_primary_sales, fact_secondary_sales]  # Different grain
```

### 5.3 How Semantic Layer Validates Queries

**Example:** "Show me velocity for new products by region"

```python
# Step 1: Resolve synonyms
"velocity" â†’ "sales_velocity"
"new products" â†’ filter on product_lifecycle = 'new'
"region" â†’ dimension "region"

# Step 2: Check compatibility
âœ“ sales_velocity allows dimension "region"
âœ“ Metric grain (weekly) compatible with region
âœ“ User has access to "sales_velocity"
âœ“ User has access to "region"

# Step 3: Check dependencies
sales_velocity requires:
  âœ“ fact_secondary_sales
  âœ“ dim_distribution
  âœ“ dim_product (for new products filter)
  âœ“ dim_geography (for region)

# Step 4: Determine join path
fact_secondary_sales
  â†’ dim_product (for lifecycle filter)
  â†’ dim_geography (for region dimension)
  â†’ dim_distribution (for stores_carrying)

# Step 5: Apply business rules
âœ“ Exclude returns (return_flag = 'N')
âœ“ Apply user territory filter (RLS)
âœ“ Use weekly grain

# Step 6: Cost check
Estimated: ~500 regions Ã— 52 weeks = 26K rows âœ“

# Step 7: Table selection
Use: agg_weekly_sku_region (faster than 10B row fact table)

# Query approved â†’ Generate SQL
```

---

## 6. CPG Industry Implementation

### 6.1 CPG-Specific Metrics

#### Primary Business Metrics

```python
CPG_METRICS = {
    "secondary_sales_value": {
        "formula": "SUM(invoice_value)",
        "filters": ["return_flag = 'N'", "transaction_type = 'Secondary'"],
        "description": "Sales from distributor to retailer",
        "format": "${:,.0f}"
    },
    
    "secondary_sales_volume": {
        "formula": "SUM(invoice_quantity)",
        "description": "Units sold distributor â†’ retailer",
        "format": "{:,.0f} units"
    },
    
    "sales_velocity": {
        "formula": "SUM(units) / COUNT(DISTINCT stores_carrying) / weeks",
        "description": "Units per store per week",
        "format": "{:.2f} units/store/week"
    },
    
    "numeric_distribution": {
        "formula": "COUNT(DISTINCT stores_with_sku) / total_stores * 100",
        "description": "% of stores carrying at least one SKU",
        "format": "{:.1f}%"
    },
    
    "acv_distribution": {
        "formula": "SUM(store_acv WHERE has_sku) / SUM(store_acv) * 100",
        "description": "% of All Commodity Volume in stores with product",
        "format": "{:.1f}%"
    },
    
    "days_of_stock": {
        "formula": "current_inventory / avg_daily_sales",
        "description": "Average inventory days on hand",
        "format": "{:.1f} days"
    },
    
    "zero_billing_outlets": {
        "formula": "COUNT(outlets WHERE sales_last_30_days = 0)",
        "description": "Outlets with no sales in last 30 days",
        "format": "{:,} outlets"
    },
    
    "market_share": {
        "formula": "our_sales / total_category_sales * 100",
        "description": "Our sales as % of total category",
        "data_sources": ["internal", "syndicated_data"],
        "format": "{:.1f}%"
    }
}
```

#### Growth & Comparison Metrics

```python
GROWTH_METRICS = {
    "growth_mom": {
        "formula": "(current_month - prior_month) / prior_month * 100",
        "comparison_type": "period",
        "comparison_offset": "-1 month",
        "format": "{:+.1f}%"
    },
    
    "growth_yoy": {
        "formula": "(current_period - year_ago) / year_ago * 100",
        "comparison_type": "period",
        "comparison_offset": "-52 weeks",
        "format": "{:+.1f}%"
    },
    
    "growth_wow": {
        "formula": "(current_week - prior_week) / prior_week * 100",
        "comparison_type": "period",
        "comparison_offset": "-1 week",
        "format": "{:+.1f}%"
    },
    
    "vs_target": {
        "formula": "(actual - target) / target * 100",
        "comparison_source": "planning_system",
        "format": "{:+.1f}%"
    }
}
```

### 6.2 CPG Dimension Hierarchies

#### Product Hierarchy
```
Category (50)
  â””â”€ Brand (500)
      â””â”€ SKU (10,000)
      
Attributes:
- launch_date
- lifecycle_stage: new/growth/mature/decline
- price_tier: premium/mid/value
- pack_size
- velocity_segment: A/B/C
```

#### Geography Hierarchy
```
Zone (4)
  â””â”€ Region (20)
      â””â”€ State (50)
          â””â”€ Territory (200)
          
Synonyms:
- region: ["area", "zone"]
- territory: ["beat", "route"]
```

#### Customer Hierarchy
```
Channel (10): GT/MT/E-Commerce/Foodservice
  â””â”€ Distributor (500)
      â””â”€ Retailer (5,000)
          â””â”€ Outlet (50,000)
```

#### Time Hierarchy (Fiscal)
```
Fiscal Year
  â””â”€ Fiscal Quarter
      â””â”€ Fiscal Period (4-4-5 pattern)
          â””â”€ Fiscal Week
              â””â”€ Date
```

### 6.3 CPG Business Rules

```python
class CPGBusinessRules:
    """Industry-specific logic"""
    
    @staticmethod
    def classify_product_lifecycle(launch_date):
        """Auto-classify product age"""
        months = (now - launch_date).days / 30
        if months < 6: return "new"
        if months < 18: return "growth"
        if months < 60: return "mature"
        return "decline"
    
    @staticmethod
    def classify_velocity_segment(velocity, category_avg):
        """ABC classification"""
        if velocity >= category_avg * 1.5: return "A"
        if velocity >= category_avg * 0.5: return "B"
        return "C"
    
    @staticmethod
    def detect_stockout_risk(days_of_stock, replenish_days=7):
        """Risk classification"""
        if days_of_stock < replenish_days: return "high_risk"
        if days_of_stock < replenish_days * 2: return "medium_risk"
        return "low_risk"
    
    @staticmethod
    def calculate_ppi(promoted_sales, base_sales, discount_pct):
        """Promotional Performance Index"""
        sales_lift = (promoted_sales / base_sales) - 1
        return sales_lift / discount_pct if discount_pct > 0 else 0
```

---

## 7. Query Intelligence System

### 7.1 The Eight Query Archetypes

Business questions fall into finite patterns:

```python
class QueryIntent(Enum):
    """Finite set of query patterns"""
    
    TREND = "trend"
    # "Show sales over time"
    # Pattern: Metric + Time + Optional filters
    # Output: Time-series data
    
    COMPARISON = "comparison"
    # "Compare this month vs last month"
    # Pattern: Metric + Baseline + Optional dimensions
    # Output: Growth %, delta, or ratio
    
    RANKING = "ranking"
    # "Top 10 products by sales"
    # Pattern: Metric + Dimension + Limit
    # Output: Sorted list with rank
    
    SNAPSHOT = "snapshot"
    # "What were sales yesterday?"
    # Pattern: Metric + Specific time point
    # Output: Single value or simple table
    
    DIAGNOSTIC = "diagnostic"
    # "Why did sales drop?"
    # Pattern: Multi-query workflow
    # Output: Root cause analysis report
    
    CONTRIBUTION = "contribution"
    # "What % of sales came from Brand X?"
    # Pattern: Part-to-whole calculation
    # Output: Percentage breakdown
    
    MIX_SHIFT = "mix_shift"
    # "How did product mix change?"
    # Pattern: Composition over time
    # Output: Percentage changes by component
    
    EXCEPTION = "exception"
    # "Which territories missed target?"
    # Pattern: Threshold-based filtering
    # Output: Items outside normal range
```

### 7.2 Query Grammar

Think of this as the "syntax" of business questions:

```
Query = Intent + Metric + [Dimensions] + [Filters] + [Time] + [Options]

Examples:

TREND:
  "Show [metric] by [time_dimension] for [time_range]"
  â†’ secondary_sales_value by week for last_13_weeks

COMPARISON:
  "Compare [metric] [current] vs [baseline]"
  â†’ secondary_sales_value this_month vs prior_month

RANKING:
  "Top [N] [dimension] by [metric]"
  â†’ Top 10 SKUs by sales_velocity

DIAGNOSTIC:
  "Why did [metric] [change_direction] in [scope]?"
  â†’ Why did sales drop in Northeast region?
```

### 7.3 Intent Classification Examples

```python
# User questions and their structured intents:

examples = {
    "Show beverage sales by week for last quarter": {
        "intent": "trend",
        "metric": "secondary_sales_value",
        "dimensions": ["fiscal_week"],
        "filters": {"category": "Beverages"},
        "time_window": "last_quarter"
    },
    
    "How did sales this month compare to last month?": {
        "intent": "comparison",
        "metric": "secondary_sales_value",
        "comparison": {
            "type": "period",
            "baseline": "previous_period",
            "metric_variant": "growth"
        }
    },
    
    "Top 10 SKUs by velocity in Tamil Nadu": {
        "intent": "ranking",
        "metric": "sales_velocity",
        "dimensions": ["sku"],
        "filters": {"state": "Tamil Nadu"},
        "limit": 10,
        "sort": "DESC"
    },
    
    "What were sales yesterday in the Northeast?": {
        "intent": "snapshot",
        "metric": "secondary_sales_value",
        "filters": {"region": "Northeast"},
        "time_point": "yesterday"
    },
    
    "Why did beverage sales drop in Q4?": {
        "intent": "diagnostic",
        "metric": "secondary_sales_value",
        "anomaly": "drop",
        "filters": {"category": "Beverages"},
        "time_window": "Q4"
    }
}
```

---

## 8. Multi-Query Diagnostic Engine

### 8.1 The Strategic Differentiator

When users ask **"Why did sales drop?"**, they need **root cause analysis**, not just data.

**Single Query Systems Say:**
"Sales dropped 8%."

**Our Diagnostic Engine Says:**
```
Sales dropped 8% in Northeast beverages due to:

PRIMARY DRIVERS:
1. Distribution Loss (-12 points numeric distribution)
   â†’ 15 stores discontinued SKU-A
   â†’ Supply chain disruption in Week 42

2. SKU-A Performance (-20% sales)
   â†’ Accounted for 45% of total decline
   â†’ Lost promotional support

3. Competitive Pressure (+8% competitor pricing advantage)
   â†’ Competitor B reduced price by 8%
   â†’ Our sales -15% in stores with competitive promo

RECOMMENDATIONS:
1. Restore distribution in 15 lost stores (priority)
2. Review promotional calendar for SKU-A
3. Evaluate pricing strategy vs Competitor B
```

### 8.2 Diagnostic Workflow

```
User: "Why did beverage sales drop in Northeast last month?"

Orchestrator executes 6 queries in parallel:

â”Œâ”€ Query 1: Trend Analysis
â”‚  "Did sales actually drop? When did it start?"
â”‚  â†’ Confirms 8% drop, started Week 42
â”‚
â”œâ”€ Query 2: Contribution by Brand/SKU
â”‚  "Which products drove the decline?"
â”‚  â†’ Brand X (-15%), SKU-A (-20%), SKU-B (-18%)
â”‚
â”œâ”€ Query 3: Distribution Check
â”‚  "Did we lose distribution?"
â”‚  â†’ Numeric: 85% â†’ 73% (-12 points)
â”‚  â†’ ACV: 92% â†’ 88% (minor)
â”‚
â”œâ”€ Query 4: Price Analysis
â”‚  "Did pricing change?"
â”‚  â†’ Our price: $2.45 â†’ $2.48 (+1.2%)
â”‚  â†’ Competitor: $2.50 â†’ $2.30 (-8%)
â”‚
â”œâ”€ Query 5: Promotional Analysis
â”‚  "Was promotional activity different?"
â”‚  â†’ % on promo: 35% â†’ 22% (-13 points)
â”‚  â†’ PPI score: 1.8 â†’ 1.3 (less effective)
â”‚
â””â”€ Query 6: Exception Detection
   "Any outliers?"
   â†’ 12 stores: stockouts
   â†’ 5 stores: competitor promo
   â†’ 3 accounts: switched to private label

â†“

Root Cause Inference Engine analyzes all results
â†“
Prioritizes causes by impact
â†“
Generates executive summary + recommendations
```

### 8.3 Implementation

```python
class DiagnosticOrchestrator:
    """Multi-query diagnostic workflow engine"""
    
    async def diagnose_metric_change(
        self,
        metric: str,
        filters: Dict,
        time_window: str,
        change_direction: str
    ) -> DiagnosticReport:
        
        # Step 1: Confirm trend
        trend = await self.trend_analysis(metric, filters, time_window)
        
        if not self.is_significant_change(trend):
            return DiagnosticReport(
                status="NO_ANOMALY",
                message="Metric within normal range"
            )
        
        # Step 2-6: Execute diagnostic queries in parallel
        results = await asyncio.gather(
            self.contribution_analysis(metric, filters),
            self.distribution_check(filters, time_window),
            self.price_analysis(filters, time_window),
            self.promotion_analysis(filters, time_window),
            self.exception_detection(metric, filters)
        )
        
        # Step 7: Synthesize root causes
        root_causes = self.infer_root_causes(
            trend=trend,
            contribution=results[0],
            distribution=results[1],
            pricing=results[2],
            promotions=results[3],
            exceptions=results[4]
        )
        
        return DiagnosticReport(
            primary_metric_change=trend.change_pct,
            root_causes=root_causes,
            recommendations=self.generate_recommendations(root_causes),
            sub_analyses=dict(zip([
                "contribution", "distribution", "pricing", 
                "promotions", "exceptions"
            ], results))
        )
```

**Output Structure:**

```python
@dataclass
class DiagnosticReport:
    primary_metric_change: float  # -8.2%
    root_causes: List[RootCause]
    recommendations: List[str]
    sub_analyses: Dict[str, Any]
    
@dataclass
class RootCause:
    factor: str  # "Distribution Loss"
    impact_pct: float  # -12.0
    severity: str  # "HIGH" | "MEDIUM" | "LOW"
    description: str
    recommendation: str
```

---

## 9. Security & Governance Framework

### 9.1 Row-Level Security (RLS)

**Requirement:** Users should only see data they're authorized to access.

**Implementation:** Security filters automatically injected based on user role.

```python
@dataclass
class UserContext:
    user_id: str
    role: Literal["sales_rep", "manager", "director", "executive"]
    territories: List[str]
    regions: List[str]
    data_access_level: Literal["territory", "region", "zone", "national"]

class RowLevelSecurity:
    def apply_security_filters(self, intent, user):
        """Auto-inject security filters"""
        
        if user.data_access_level == "territory":
            # Sales rep sees only their territory
            intent.filters.append(Filter(
                dimension="territory",
                operator="IN",
                value=user.territories
            ))
        
        elif user.data_access_level == "region":
            # Manager sees region rollup
            intent.filters.append(Filter(
                dimension="region",
                operator="IN",
                value=user.regions
            ))
        
        # Log for audit
        audit_log.record(
            user=user.user_id,
            query=intent,
            filters_applied=intent.filters
        )
        
        return intent
```

**Example:**

Sales Rep asks: "Show me sales by store"

System automatically adds: `WHERE territory_id = 'NORTHEAST_01'`

They never see data outside their territory.

### 9.2 Audit Logging

**Every query is logged:**

```python
class AuditLogger:
    def log_query(self, query_id, user, question, intent, sql, results):
        audit_record = {
            "timestamp": datetime.utcnow(),
            "query_id": query_id,
            "user_id": user.user_id,
            "user_role": user.role,
            
            # What was asked
            "natural_language": question,
            "intent_type": intent.intent_type,
            "metrics": intent.metrics,
            "dimensions": intent.dimensions,
            
            # What was executed
            "sql_queries": sql,
            "tables_accessed": self.extract_tables(sql),
            
            # What was returned
            "row_count": len(results),
            "execution_time_ms": results.execution_time,
            
            # Security
            "security_filters": intent.filters,
            "data_access_level": user.data_access_level
        }
        
        # Write to audit database
        self.audit_db.insert(audit_record)
```

**Audit Trail Enables:**
- Compliance (SOX, GDPR)
- Security monitoring
- Usage analytics
- Troubleshooting
- Metric validation

### 9.3 Query Cost Controls

**Prevent expensive queries from running:**

```python
class QueryCostEstimator:
    LIMITS = {
        "max_result_rows": 100_000,
        "max_group_by_cardinality": 4,
        "max_scan_gb": 10.0,
        "timeout_seconds": 300
    }
    
    def estimate_and_validate(self, ast):
        # Estimate based on statistics
        estimated_rows = self.estimate_cardinality(ast)
        estimated_scan_gb = self.estimate_scan_size(ast)
        
        violations = []
        
        if estimated_rows > self.LIMITS["max_result_rows"]:
            violations.append(
                f"Query would return {estimated_rows:,} rows. "
                f"Add filters to reduce to under {self.LIMITS['max_result_rows']:,}."
            )
        
        if estimated_scan_gb > self.LIMITS["max_scan_gb"]:
            violations.append(
                f"Query would scan {estimated_scan_gb:.1f}GB. "
                f"Consider using aggregated table or reducing time range."
            )
        
        if violations:
            raise QueryTooExpensiveError(
                violations=violations,
                suggestion=self.suggest_optimization(ast)
            )
        
        return QueryCostEstimate(
            estimated_rows=estimated_rows,
            estimated_scan_gb=estimated_scan_gb,
            approved=True
        )
```

**Cost Limits by Role:**

| Role | Max Rows | Max Scan GB | Daily Quota |
|------|----------|-------------|-------------|
| Sales Rep | 10,000 | 1 GB | 10 GB |
| Manager | 100,000 | 10 GB | 100 GB |
| Executive | 1,000,000 | 100 GB | 1,000 GB |

### 9.4 Metric Governance

**Single Source of Truth:**

```
BEFORE (Chaos):
- Finance team: "Revenue = gross_sales"
- Sales team: "Revenue = net_sales" 
- Operations: "Revenue = invoice_value"
â†’ Different numbers in different reports!

AFTER (Governed):
- semantic_layer/metrics/secondary_sales_value.yaml
- ONE definition
- EVERYONE uses same calculation
- Change once, updates everywhere
```

---

## 10. Complete Technical Implementation

### 10.1 Technology Stack

#### **Core Platform**
- **Language:** Python 3.11+
- **Framework:** FastAPI (REST API)
- **LLM Integration:** Anthropic Claude API
- **Semantic Layer:** Custom (extensible to dbt/Cube.js)
- **Query Builder:** Custom AST implementation

#### **Data Layer**
- **Primary:** Snowflake / Redshift / BigQuery
- **Cache:** Redis
- **Audit DB:** PostgreSQL

#### **Infrastructure**
- **Container:** Docker
- **Orchestration:** Kubernetes
- **API Gateway:** Kong / AWS API Gateway
- **Monitoring:** Datadog / Grafana
- **Logging:** ELK Stack

#### **Frontend**
- **Web:** React
- **Mobile:** React Native
- **Chat Interfaces:** MS Teams SDK, Slack SDK

### 10.2 Key Dependencies

```python
# requirements.txt
anthropic>=0.40.0              # LLM for intent recognition
pydantic>=2.0.0                # Data validation
fastapi>=0.100.0               # API framework
uvicorn>=0.23.0                # ASGI server
sqlalchemy>=2.0.0              # Database ORM
redis>=5.0.0                   # Caching
snowflake-connector-python>=3.0 # Data warehouse
python-jose>=3.3.0             # JWT authentication
pytest>=7.0.0                  # Testing
```

### 10.3 Project Structure

```
conversational-analytics/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py                    # FastAPI application
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ query.py               # Query endpoints
â”‚   â”‚   â””â”€â”€ admin.py               # Admin endpoints
â”‚   â””â”€â”€ middleware/
â”‚       â”œâ”€â”€ auth.py                # Authentication
â”‚       â””â”€â”€ rate_limit.py          # Rate limiting
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ intent_recognizer.py      # LLM integration
â”‚   â”œâ”€â”€ semantic_validator.py     # Validation logic
â”‚   â”œâ”€â”€ ast_query_builder.py      # Query generation
â”‚   â”œâ”€â”€ diagnostic_orchestrator.py # Multi-query workflows
â”‚   â””â”€â”€ response_generator.py     # NL response generation
â”‚
â”œâ”€â”€ semantic_layer/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ semantic_schema.py    # Core data models
â”‚   â”‚   â”œâ”€â”€ cpg_metrics.py        # CPG metric definitions
â”‚   â”‚   â””â”€â”€ cpg_dimensions.py     # CPG dimension definitions
â”‚   â”œâ”€â”€ validators/
â”‚   â”‚   â””â”€â”€ semantic_validator.py
â”‚   â””â”€â”€ registry/
â”‚       â”œâ”€â”€ metrics.yaml           # Metric registry
â”‚       â””â”€â”€ dimensions.yaml        # Dimension registry
â”‚
â”œâ”€â”€ security/
â”‚   â”œâ”€â”€ rls.py                     # Row-level security
â”‚   â”œâ”€â”€ audit.py                   # Audit logging
â”‚   â””â”€â”€ cost_controls.py           # Query cost limits
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ fiscal_calendar.py         # Fiscal date handling
â”‚   â”œâ”€â”€ cache_manager.py           # Redis cache
â”‚   â””â”€â”€ query_executor.py          # DB execution
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ e2e/
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ development.yaml
â”‚   â”œâ”€â”€ staging.yaml
â”‚   â””â”€â”€ production.yaml
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ api_documentation.md
    â””â”€â”€ user_guide.md
```

### 10.4 Sample Code Snippets

#### Main API Endpoint

```python
# api/main.py
from fastapi import FastAPI, Depends
from core.intent_recognizer import IntentRecognizer
from core.semantic_validator import SemanticValidator
from core.ast_query_builder import ASTQueryBuilder
from security.rls import RowLevelSecurity
from security.audit import AuditLogger

app = FastAPI(title="Conversational Analytics API")

@app.post("/query")
async def execute_query(
    question: str,
    user: UserContext = Depends(get_current_user)
):
    """Main query endpoint"""
    
    query_id = generate_query_id()
    
    try:
        # Step 1: Extract intent from natural language
        intent = intent_recognizer.extract_intent(question)
        
        # Step 2: Validate against semantic layer
        validated_intent = semantic_validator.validate(intent)
        
        # Step 3: Apply security filters
        secured_intent = rls.apply_security_filters(validated_intent, user)
        
        # Step 4: Generate SQL via AST
        sql = query_builder.build_query(secured_intent)
        
        # Step 5: Execute
        results = await query_executor.execute(sql)
        
        # Step 6: Generate response
        response = response_generator.generate(
            question, results, secured_intent
        )
        
        # Step 7: Log for audit
        audit_logger.log_query(
            query_id, user, question, secured_intent, sql, results
        )
        
        return {
            "query_id": query_id,
            "response": response,
            "results": results,
            "metadata": {
                "row_count": len(results),
                "execution_time_ms": results.execution_time
            }
        }
    
    except Exception as e:
        audit_logger.log_failed_query(query_id, user, question, e)
        raise
```

#### Intent Recognition

```python
# core/intent_recognizer.py
class IntentRecognizer:
    def __init__(self, api_key: str):
        self.client = anthropic.Anthropic(api_key=api_key)
        self.model = "claude-sonnet-4-20250514"
    
    def extract_intent(self, question: str) -> IntentObject:
        """Extract structured intent using Claude"""
        
        system_prompt = """You extract structured analytics intent from questions.

Available Metrics:
- secondary_sales_value, sales_velocity, numeric_distribution, etc.

Available Dimensions:
- Time: date, week, month, quarter, year
- Product: category, brand, sku
- Geography: region, state, territory
- Customer: channel, distributor

Output ONLY valid JSON matching IntentObject schema."""

        response = self.client.messages.create(
            model=self.model,
            max_tokens=2000,
            temperature=0,  # Deterministic
            system=system_prompt,
            messages=[{"role": "user", "content": question}]
        )
        
        # Parse and validate
        intent_dict = json.loads(response.content[0].text)
        intent = IntentObject(**intent_dict)
        
        return intent
```

### 10.5 Deployment Configuration

#### Docker Compose

```yaml
# docker-compose.yml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}
      - REDIS_URL=redis://redis:6379
    depends_on:
      - redis
      - postgres
  
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
  
  postgres:
    image: postgres:15
    environment:
      - POSTGRES_DB=audit
      - POSTGRES_USER=admin
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
```

---

## 11. Production Deployment Guide

### 11.1 Infrastructure Setup

#### AWS Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USERS                                 â”‚
â”‚  Web App â”‚ Mobile App â”‚ MS Teams â”‚ Slack                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTPS
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AWS CloudFront (CDN)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           AWS API Gateway                                    â”‚
â”‚  â€¢ Authentication (Cognito)                                  â”‚
â”‚  â€¢ Rate Limiting                                             â”‚
â”‚  â€¢ Request/Response logging                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        AWS ECS (Fargate) - Auto-scaling                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Conversational Analytics Service (Containers)       â”‚   â”‚
â”‚  â”‚  â€¢ Intent Recognition                                â”‚   â”‚
â”‚  â”‚  â€¢ Query Building                                    â”‚   â”‚
â”‚  â”‚  â€¢ Response Generation                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚
        â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ElastiCache   â”‚         â”‚   RDS         â”‚
â”‚ (Redis)       â”‚         â”‚ (PostgreSQL)  â”‚
â”‚ â€¢ Query cache â”‚         â”‚ â€¢ Audit logs  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Snowflake Data Warehouse                        â”‚
â”‚  â€¢ Fact tables                                               â”‚
â”‚  â€¢ Dimension tables                                          â”‚
â”‚  â€¢ Aggregated tables                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 11.2 Deployment Steps

#### Phase 1: Infrastructure (Week 1)

```bash
# 1. Setup AWS resources
terraform init
terraform plan
terraform apply

# 2. Configure Snowflake
snowsql -f setup/snowflake_schema.sql

# 3. Deploy Redis cache
aws elasticache create-cache-cluster \
  --cache-cluster-id conv-analytics-cache \
  --engine redis

# 4. Setup RDS for audit
aws rds create-db-instance \
  --db-instance-identifier audit-db \
  --engine postgres
```

#### Phase 2: Application Deployment (Week 2)

```bash
# 1. Build Docker image
docker build -t conv-analytics:latest .

# 2. Push to ECR
aws ecr get-login-password | docker login
docker push ${ECR_REPO}/conv-analytics:latest

# 3. Deploy to ECS
aws ecs update-service \
  --cluster conv-analytics \
  --service api \
  --force-new-deployment

# 4. Configure API Gateway
aws apigateway create-rest-api \
  --name ConversationalAnalytics
```

#### Phase 3: Monitoring Setup (Week 2)

```bash
# 1. CloudWatch dashboards
aws cloudwatch put-dashboard \
  --dashboard-name ConvAnalytics \
  --dashboard-body file://monitoring/dashboard.json

# 2. Alarms
aws cloudwatch put-metric-alarm \
  --alarm-name high-error-rate \
  --metric-name Errors \
  --threshold 10

# 3. Logging
aws logs create-log-group \
  --log-group-name /ecs/conv-analytics
```

### 11.3 Monitoring & Alerting

#### Key Metrics to Track

```python
METRICS = {
    "performance": {
        "query_latency_p50": "< 2 seconds",
        "query_latency_p95": "< 5 seconds",
        "query_latency_p99": "< 10 seconds"
    },
    "availability": {
        "uptime": "> 99.5%",
        "error_rate": "< 1%"
    },
    "business": {
        "daily_active_users": "track growth",
        "queries_per_user_per_day": "track engagement",
        "query_success_rate": "> 85%"
    },
    "cost": {
        "snowflake_credits_per_day": "monitor spend",
        "cache_hit_rate": "> 60%"
    }
}
```

#### Alert Rules

```yaml
alerts:
  - name: HighErrorRate
    condition: error_rate > 5%
    severity: critical
    action: page_oncall
  
  - name: SlowQueries
    condition: p95_latency > 10s
    severity: warning
    action: notify_team
  
  - name: LowCacheHit
    condition: cache_hit_rate < 40%
    severity: info
    action: log_metric
```

---

## 12. Business Case & ROI

### 12.1 Current State Analysis

**Org Size:** 500 sales reps, 50 managers, 10 directors

**Current Pain Points:**

| Problem | Time Impact | Annual Cost |
|---------|-------------|-------------|
| Manual report creation | 8 hrs/week per rep | $10.4M |
| Waiting for analyst support | 2 hrs/week per rep | $2.6M |
| Inconsistent metrics | 20% of reports redone | $2.1M |
| Delayed decisions | Missed opportunities | $5M+ |
| **TOTAL** | | **~$20M/year** |

**Calculation:**
- 500 reps Ã— 8 hours/week Ã— 52 weeks Ã— $100/hour = $20.8M
- Plus: opportunity cost of slow decisions

### 12.2 Solution Benefits

#### Quantitative Benefits

**Time Savings:**
- Sales reps: 8 hours/week â†’ 1 hour/week (87.5% reduction)
- Managers: 3 hours/week â†’ 0.5 hours/week (83% reduction)
- **Total savings: 4,000 hours/week**

**Cost Avoidance:**
- $150/hour Ã— 4,000 hours/week = **$600K/week**
- **Annual: ~$31 Million**

**Additional Benefits:**
- Faster decisions â†’ Capture opportunities faster
- Consistent metrics â†’ Better coordination
- Self-service â†’ Scale without adding analysts

#### Qualitative Benefits

1. **Competitive Advantage**
   - Respond to market changes in hours, not days
   - Field sales equipped with instant answers

2. **Data Democratization**
   - Everyone can access insights
   - No SQL knowledge required

3. **Improved Governance**
   - Single source of truth
   - Complete audit trail
   - Enforced security

4. **Scalability**
   - Add users without adding support staff
   - Consistent experience across organization

### 12.3 Implementation Investment

#### Development Costs

| Phase | Duration | Resources | Cost |
|-------|----------|-----------|------|
| Phase 1: Core CPG | 6 weeks | 2 engineers | $90K |
| Phase 2: Enterprise Features | 4 weeks | 2 engineers | $60K |
| Phase 3: Optimization | 3 weeks | 2 engineers | $45K |
| Infrastructure Setup | 2 weeks | 1 devops | $20K |
| **TOTAL DEVELOPMENT** | **15 weeks** | | **$215K** |

#### Ongoing Costs

| Item | Annual Cost |
|------|-------------|
| Cloud infrastructure (AWS) | $120K |
| Snowflake compute | $200K |
| Claude API (LLM) | $50K |
| Maintenance (1 engineer) | $200K |
| **TOTAL ANNUAL** | **$570K** |

### 12.4 ROI Analysis

```
Year 1:
  Investment: $215K (dev) + $570K (annual) = $785K
  Benefit: $31M (time savings + opportunity value)
  ROI: ($31M - $0.785M) / $0.785M = 3,850%
  
  Simplified: 50x return in Year 1

Years 2+:
  Annual cost: $570K
  Annual benefit: $31M
  ROI: 54x per year
```

**Payback Period:** < 2 weeks

### 12.5 Risk-Adjusted Value

**Conservative Estimates:**

Assume only 50% adoption and 50% of projected benefits:
- Benefit: $31M Ã— 0.5 Ã— 0.5 = $7.75M/year
- Cost: $785K (Year 1)
- ROI: Still 10x in Year 1

**Even in worst case, project delivers strong ROI.**

### 12.6 Success Metrics

#### Phase 1 (Pilot - Months 1-3)
- **Target:** 20 users
- **Adoption:** >15 users active weekly (75%)
- **Query success:** >80%
- **User satisfaction:** >3.5/5

#### Phase 2 (Expansion - Months 4-6)
- **Target:** 100 users
- **Adoption:** >60 users active weekly (60%)
- **Query success:** >85%
- **User satisfaction:** >4.0/5

#### Phase 3 (Full Rollout - Months 7-12)
- **Target:** 500 users
- **Adoption:** >300 users active weekly (60%)
- **Query success:** >90%
- **User satisfaction:** >4.2/5
- **Time savings:** 5-8 hours/week per user

---

## 13. Conclusion & Next Steps

### 13.1 What We've Built

A **production-ready Conversational Analytics Platform** that:

âœ… **Solves Real Business Problems**
- Saves 8 hours/week per sales rep
- Provides instant answers to business questions
- Enables self-service analytics

âœ… **Is Architecturally Sound**
- Semantic Layer encodes business logic
- LLMs used only for intent understanding
- AST-based query generation (zero hallucination)
- Complete security and governance

âœ… **Delivers CPG-Specific Value**
- Secondary sales, velocity, distribution metrics
- Fiscal calendar support
- Multi-query diagnostic workflows
- Root cause analysis

âœ… **Scales for Enterprise**
- Row-level security
- Audit logging
- Cost controls
- High availability architecture

### 13.2 Why This Approach Works

**Three Core Principles:**

1. **Semantic Layer > LLM Choice**
   - Business logic is your IP
   - LLM is replaceable
   - Semantic layer is durable

2. **Finite Grammar â†’ Infinite Flexibility**
   - 8 intent types Ã— 30 metrics Ã— 15 dimensions
   - Thousands of combinations from finite rules
   - Predictable, testable, governable

3. **Diagnostic Workflows = Strategic Value**
   - "Why" questions trigger multi-query analysis
   - This is what makes it strategic, not just fancy BI

### 13.3 Recommended Next Steps

#### Immediate (Week 1)
1. âœ… **Stakeholder Review** - Present to leadership
2. âœ… **Budget Approval** - Secure $215K development budget
3. âœ… **Team Formation** - Assign 2 engineers + 1 devops

#### Short-term (Weeks 2-8)
4. âœ… **Phase 1 Implementation**
   - CPG metrics
   - Fiscal calendar
   - Basic query types

5. âœ… **Pilot Program**
   - Select 20 champion users
   - Gather feedback
   - Iterate

#### Medium-term (Weeks 9-16)
6. âœ… **Phase 2 Implementation**
   - Row-level security
   - Diagnostic workflows
   - Audit logging

7. âœ… **Expand Pilot**
   - 100 users
   - Multiple territories
   - Refine semantic layer

#### Long-term (Months 5-12)
8. âœ… **Full Rollout**
   - 500 users
   - Production support
   - Continuous improvement

9. âœ… **Measure Success**
   - Track adoption metrics
   - Calculate actual ROI
   - Identify expansion opportunities

### 13.4 Critical Success Factors

**Will Succeed If:**
- âœ… Executive sponsorship
- âœ… User champions identified
- âœ… Semantic layer prioritized over features
- âœ… Pilot before full rollout
- âœ… Feedback loop established

**Will Struggle If:**
- âŒ Treated as IT project (not business transformation)
- âŒ Skip semantic layer definition
- âŒ Launch without pilot
- âŒ Ignore user feedback

### 13.5 Final Recommendation

**Proceed with Phase 1 implementation** (6 weeks, $90K investment)

This will:
- Validate architecture
- Build CPG-specific capabilities
- Enable pilot program
- Prove value before full investment

**Expected Outcome:** Pilot users saving 5-8 hours/week, query success rate >80%, path to full rollout clear.

---

## Appendix

### A. Glossary of Terms

**AST (Abstract Syntax Tree):** Structured representation of SQL query  
**CPG (Consumer Packaged Goods):** Industry (food, beverage, household products)  
**RLS (Row-Level Security):** User-specific data access controls  
**Semantic Layer:** Business logic layer between users and data  
**Secondary Sales:** Distributor â†’ Retailer transactions  
**Numeric Distribution:** % of stores carrying a product  
**Sales Velocity:** Units sold per store per time period  

### B. Reference Architecture Diagram

See Section 4.1 for complete system architecture.

### C. Sample Queries Library

See Section 7.3 for query examples by intent type.

### D. API Documentation

Available at: `/docs/api_documentation.md`

### E. Contact Information

**Document Author:** Varun - Senior Data Engineer  
**For Technical Questions:** See implementation documentation  
**For Business Questions:** Contact project sponsor  

---

**Document Version:** 1.0  
**Last Updated:** February 2026  
**Status:** âœ… Production Ready

---

**END OF DOCUMENT**
